{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutionnal Generative Adversarial Network (DCGAN)\n",
    "# DL Lecture with Pr. Kelly Joly\n",
    "\n",
    "### Aymeric MILLAN & Arthur VIENS\n",
    "\n",
    "In this notebook we are going to present our DCGAN. Its purpose is to generate\n",
    "fake images that look like real images, after training on a particular dataset. \n",
    "We were interested in GANs because we thought it would be really interesting to \n",
    "dive into the details of training one. For other types of deep learning \n",
    "architectures, it is pretty straightforward to train a network, but that is not\n",
    "the case with GANs.\n",
    "\n",
    "Here is an example of three generated pictures of resolution `128x128`. This\n",
    "training has been done with a [landscapes dataset](google.com). The rendering is\n",
    "not perfect at all, and couldn't fool a human discriminator, but we can see that\n",
    "it is starting to _look like_ a landscape. \n",
    "\n",
    "![Example of generated images](fig/sluggy_landscapes.png)\n",
    "\n",
    "We faced many various difficulties while implementing this GAN, which we are\n",
    "going to present in this Notebook. The problems we encoutered mainly came from\n",
    "two sides :\n",
    "- Architecture : These were all the \"inner\" problems, which are directly related\n",
    "to the network (e.g., which convolutionnal layers to use, how to avoid gradient\n",
    "vanishing, what size of upsampling, etc.)\n",
    "- Training : These are the \"outer\" problems, which are not directly related to\n",
    "the architecture of our GAN, but mostly about the training. For example, setting\n",
    "the right learning rate, or choosing the frequency of training of the generator\n",
    "regarding the discriminator, etc.\n",
    "\n",
    "Our training was executed on NVIDIA's last generation GPUs, `A100`. Even with\n",
    "such computational power, training our network took quite a long time. To be\n",
    "able to _start_ to see some result, and taking decision on adjusting our\n",
    "network, we nedded at least 10-12 hours. And this is for `128x128` images.\n",
    "We will discuss about scaling up our GAN later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders and Datasets\n",
    "\n",
    "The first part of the project was to be able to load correctly our datasets. For\n",
    "this we used the different utility classes of PyTorch such as Dataset and\n",
    "DataLoader from `torch.utils.data`. With these classes, we just have to\n",
    "implement some methods to retrieve an item from the dataset and to get the total\n",
    "length of the dataset, and we can use the whole system behind it.\n",
    "For example, it is possible to use this machinery to do shuffling, multiprocess\n",
    "loading, batch prefetching, dataset weighting and much more.\n",
    "\n",
    "Moreover, we used different image transforms from `torchvision.transform`, such\n",
    "as resizing, cropping, horizontal flipping or transforms composition for\n",
    "example. It was really useful to use this kind of transformations for data\n",
    "augmentation. Hence, the images from dataset are not always exactly the same and\n",
    "it makes it harder for the network to overfit on the training set.\n",
    "\n",
    "(IMAGE_ORIGINALE_ET_COMPARAISON_IMAGE_FLIP_OU_AGGRANDIE_etc)[IMAGE]\n",
    "\n",
    "As the landscapes dataset's size is more than 10 GB, it can not fit in memory at \n",
    "once. Hence, it is usefull to load images on the fly and compute the \n",
    "transformations at the same time. To achieve this, we tuned the\n",
    "`prefecth factor` and the number of workers to use the full capacity of the\n",
    "available GPUs.\n",
    "\n",
    "In our actual code, the pipeline of data loading is :\n",
    "\n",
    "1. Selection of a random image\n",
    "2. Read image as 3d matrix\n",
    "3. Resize image\n",
    "4. Crop image to the size we want to generate\n",
    "5. Transform to a PyTorch tensor\n",
    "6. 50% chance of horizontal flip the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "For our GAN project, we started off very simple. Our first network was a fully \n",
    "connected GAN creating `28x28` images of black and white numbers, as we trained \n",
    "first of MNIST dataset. We increased the complexity afterwards, replacing \n",
    "fully-connected linear layers with convolutional layers, transposed \n",
    "convolutions and upsampling layers. It is pretty simple for a GAN to generate \n",
    "small images such as MNIST digits, but as the resolution increases, it becomes\n",
    "harder and harder to have satisfactory results. The scaling up of a GAN is not\n",
    "a trivial task.\n",
    "\n",
    "Our last architecture is composed of many _ResBlocks_ and _ResUpBlocks_, in order \n",
    "to keep the gradient flowing through the layers via the skip connections. \n",
    "Our networks are pretty deep, and it is important for the gradient to flow, \n",
    "else the training fails or takes an extremely long time. Our ResBlock is built \n",
    "as such : __METTRE UN DIAGRAMME__\n",
    "\n",
    "And our ResUpBlock, which is the ResBlock counterpart that scale the images up\n",
    "instead of scaling them down, is composed like this : __METTRE UN DIAGRAMME__\n",
    "\n",
    "We saw on the litterature different ways to implement the ResBlock, but we \n",
    "chose one that we thought was best for us, and eventually, we engineered it even\n",
    "more to best fit our needs.\n",
    "\n",
    "Our whole architeture depends on the size of the images we want to generate.\n",
    "Of course, a network will need more parameters to generate `256x256` images than\n",
    "`128x128` images, so the architecture is similar, but we add additional layers\n",
    "for `256x256` generation. DIAGRAMME ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training strategies\n",
    "\n",
    "Training a GAN is not straightforwards. This process is a zero-sum game between \n",
    "the generator and the discriminator. Because of this, they are able to train \n",
    "each other as they take turn in winning the game. It needs a complicated \n",
    "equilibrium in order to succesfully generate images that feel real for human \n",
    "eye. If one of them is too strong for the other, it generally makes the other \n",
    "one collapse, and training fails. It is of paramount importance to train them \n",
    "in such a way that they are approximately at the same level. <br/>\n",
    "\n",
    "GANs are trained in the following fashion : In each training loops, we sample\n",
    "first a batch of points from the latent space, that we feed to the generator\n",
    "that outputs a batch of fake images. We feed them to the discriminator, that\n",
    "has to label them as real or fake. Obviously, its goal is to label the\n",
    "generator images as fake. We then feed a batch of images from the dataset to\n",
    "the discriminator, that has to label them aswell. Once the training step of the\n",
    "discriminator is finished, we train the generator. After sampling a batch of\n",
    "latent points and feeding them to the generator, we freeze the discriminator\n",
    "weights and ask it to label the fake images. Here, the goal of the generator is\n",
    "to have its images labeled as real, and we compute the loss accordingly.\n",
    "\n",
    "There are different variants of how we execute this training, and different ways\n",
    "to tweak the training. For example we can :\n",
    "- Train k times the discriminator and only once the generator (k is typically \n",
    "low : 2 or 3)\n",
    "- Use a different learning rate for each network\n",
    "- Add noise the to real and generated images\n",
    "- Change the latent space dimension (not trivial, the whole network\n",
    "architecture needs to be able to convert it to an image)\n",
    "- Perform \"label smooting\", by setting the label of real images to 0.9 instead\n",
    "of 1 for example, in order regularize the discriminator by making it less \n",
    "overconfident of his predictions\n",
    "- Tune the different hyperparameters\n",
    "- Regularize the networks with L1, L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Size\n",
    "# Latent Space\n",
    "# Architecture\n",
    "Parler des paramètres \"internes\" au réseau, et des \"externes\" (learning rate, etc...)\n",
    "# Training strategies\n",
    "Entrainer un sur deux le discriminator ou generator\n",
    "# Environment setup\n",
    "(option --midsave, --resume, automatiser et profesionnaliser le tout car sinon plus possible d'améliorer)\n",
    "# Artifacts\n",
    "\n",
    "# Montrer images a différentes epochs\n",
    "\n",
    "# Parler du papier sur le BIGGAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aff6a0c97bf647d57f829f973db0d341d149cf8adebb692db2ac85668315af91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
