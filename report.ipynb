{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutionnal Generative Adversarial Network (DCGAN)\n",
    "# DL Lecture with Pr. Kelly Joly\n",
    "\n",
    "### Aymeric MILLAN & Arthur VIENS\n",
    "\n",
    "In this notebook we are going to present our DCGAN. Its purpose is to generate\n",
    "fake images that look like real images, after training on a particular dataset. \n",
    "We were interested in GANs because we thought it would be really interesting to \n",
    "dive into the details of training one. For other types of deep learning \n",
    "architectures, it can be pretty straightforward to train a network, but that is \n",
    "not the case with GANs.\n",
    "\n",
    "Here is an example of three generated pictures of resolution `128x128`. This\n",
    "training has been done with a [landscapes dataset](google.com). The rendering is\n",
    "not perfect at all, and couldn't fool a human discriminator, but we can see that\n",
    "it is starting to _look like_ a landscape. \n",
    "\n",
    "![Example of generated images](fig/sluggy_landscapes.png)\n",
    "\n",
    "Our training was executed on NVIDIA's last generation GPUs, `A100`. Even with\n",
    "such computational power, training our network took quite a long time. To be\n",
    "able to _start_ to see some result, and taking decision on adjusting our\n",
    "network, we nedded at least 10-12 hours. And this is for `128x128` images.\n",
    "We will discuss about scaling up our GAN later.  \n",
    "\n",
    "This report is organized as following. Section I discusses the datasets and data loaders before section II explains our architecture. We talk about our training strategies in section III, the problems we encountered in section IV, what could be improved in section V and a demonstration on section VI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data Loaders and Datasets\n",
    "\n",
    "The first part of the project was to be able to load correctly our datasets. For\n",
    "this, we used the different utility classes of PyTorch such as `Dataset` and\n",
    "`DataLoader` from `torch.utils.data`. With these classes, we just have to\n",
    "implement some methods to retrieve an item from the dataset and to get the total\n",
    "length of the dataset, and we can use the whole system behind it.\n",
    "For example, it is possible to use this machinery to do shuffling, multiprocess\n",
    "loading, batch prefetching, dataset weighting and much more.\n",
    "\n",
    "Moreover, we used different image transforms from `torchvision.transform`, such\n",
    "as resizing, cropping, horizontal flipping or transforms composition for\n",
    "example. It was really useful to use this kind of transformations for data\n",
    "augmentation. Hence, the images from dataset are not always exactly the same and\n",
    "it makes it harder for the network to overfit on the training set.\n",
    "\n",
    "Here is an example of how we can generate multiple pictures with a simple crop\n",
    "or flip :\n",
    "\n",
    "<!-- ![Example of data augmentation](fig/data_aug.png) -->\n",
    "<img src=\"fig/data_augm.png\" alt=\"Example of data augmentation\" width=\"910\"/>\n",
    "\n",
    "As the landscapes dataset's size is more than 10 GB, it can not fit in memory at \n",
    "once. Hence, it is usefull to load images on the fly and compute the \n",
    "transformations at the same time. To achieve this, we tuned the\n",
    "`prefetch factor` and the number of workers to use the full capacity of the\n",
    "available GPUs.\n",
    "\n",
    "In our actual code, the pipeline of data loading is :\n",
    "\n",
    "1. Selection of a random image\n",
    "2. Read image as 3d matrix\n",
    "3. Resize image\n",
    "4. Crop image to the size we want to generate\n",
    "5. Transform to a PyTorch tensor\n",
    "6. 50% chance of horizontal flip the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Architecture\n",
    "\n",
    "At first, we began by modeling an autoencoder, because downsampling images \n",
    "and upsampling them is not very easy when you are a beginner, and the results\n",
    "of autoencoders are immediately visible. When our building blocks were ready,\n",
    "we began the construction of our GAN architecture based on the AE's architecture.\n",
    "\n",
    "We started off very simple. Our first network was a fully \n",
    "connected GAN creating `28x28` images of black and white numbers, as we trained \n",
    "first on MNIST dataset. We increased the complexity afterwards, replacing \n",
    "fully-connected linear layers with convolutional layers, transposed \n",
    "convolutions and upsampling layers. It is pretty simple for a GAN to generate \n",
    "small images such as MNIST digits. But as the resolution increases, it becomes\n",
    "harder and harder to have satisfactory results. The scaling up of a GAN is not\n",
    "a trivial task.\n",
    "\n",
    "Our last architecture is composed of many _ResBlocks_ and _ResUpBlocks_, in order \n",
    "to keep the gradient flowing through the layers via the skip connections. \n",
    "Our networks are quite deep, and it is important for the gradient to flow, \n",
    "else the training fails or takes an extremely long time. Our ResBlock is built \n",
    "as such : \n",
    "\n",
    "![Resblock example](fig/resblock.png)\n",
    "\n",
    "Our ResBlock uses **convolutional layers** to scale down the image, and our\n",
    "ResUpBlock use **upsampling** and **convolutional layers** to scale up the image.\n",
    "\n",
    "If the layer is supposed to scale up or down the image size, or to change the\n",
    "number of channels, the identity is replaced with a convolutional or an\n",
    "upsampling layer to be able to add the different paths of the forward pass.\n",
    "\n",
    "We saw on the litterature different ways to implement the ResBlock, but we \n",
    "chose one that we thought was best for us, and eventually, we engineered it even\n",
    "more to best fit our needs.\n",
    "\n",
    "Our whole architeture depends on the size of the images we want to generate.\n",
    "Of course, a network will need more parameters to generate `256x256` images than\n",
    "`128x128` images, so the architecture is similar, but we add additional layers\n",
    "for `256x256` generation.\n",
    "\n",
    "# DIAGRAMME ET NOMBRE DE PARAMETRES FINAL ? \n",
    "# voir dans le draw io, sinon juste mettre la phrase suivante :\n",
    "Our final model has X parameters for the Generator and X parameters for the\n",
    "discriminator, you can check the architecture in the `gan_architecture.py` file.\n",
    "\n",
    "Additionnaly, we tried to add building blocks from the SAGAN architecture\n",
    "[(Zhang et al)](https://arxiv.org/abs/1805.08318), namely the self-attention\n",
    "layer aswell as the spectral normalization, but it did not improve our generations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Training strategies\n",
    "\n",
    "Training a GAN is not straightforwards. This process is a zero-sum game between \n",
    "the generator and the discriminator. Because of this, they are able to train \n",
    "each other as they take turn in winning the game. It needs a complicated \n",
    "equilibrium in order to succesfully generate images that feel real for human \n",
    "eye. If one of them is too strong for the other, it generally makes the other \n",
    "one collapse, and training fails. It is of paramount importance to train them \n",
    "in such a way that they are approximately at the same level. <br/>\n",
    "\n",
    "GANs are trained in the following fashion : In each training loops, we sample\n",
    "first a batch of points from the latent space, that we feed to the generator\n",
    "that outputs a batch of fake images. We feed them to the discriminator, that\n",
    "has to label them as real or fake. Obviously, its goal is to label the\n",
    "generator images as fake. We then feed a batch of images from the dataset to\n",
    "the discriminator, that has to label them aswell. Once the training step of the\n",
    "discriminator is finished, we train the generator. After sampling a batch of\n",
    "latent points and feeding them to the generator, we freeze the discriminator\n",
    "weights and ask it to label the fake images. Here, the goal of the generator is\n",
    "to have its images labeled as real, and we compute the loss accordingly.\n",
    "\n",
    "The following [diagram](https://developers.google.com/machine-learning/gan/discriminator) \n",
    "summarize this process : \n",
    "\n",
    "<img src=\"fig/google_diagram.svg\" alt=\"Training of a GAN\" width=\"700\"/>\n",
    "\n",
    "There are different variants of how we execute this training, and different ways\n",
    "to tweak the training. For example we can :\n",
    "- Train k times the discriminator and only once the generator (k is typically \n",
    "low : 2 or 3)\n",
    "- Use a different learning rate for each network\n",
    "- Add noise the to real and generated images\n",
    "- Change the latent space dimension (not trivial, the whole network\n",
    "architecture needs to be able to convert it to an image)\n",
    "- Perform \"label smooting\", by setting the label of real images to 0.9 instead\n",
    "of 1 for example, in order regularize the discriminator by making it less \n",
    "overconfident of his predictions\n",
    "- Tune the different hyperparameters\n",
    "- Regularize the networks with L1, L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Problems we encountered\n",
    "\n",
    "We faced many various difficulties while implementing this GAN, which we are\n",
    "going to present in this section. The problems we encoutered mainly came from\n",
    "two sides :\n",
    "- Architecture : These were all the \"inner\" problems, which are directly related\n",
    "to the network (e.g., which convolutionnal layers to use, how to avoid gradient\n",
    "vanishing, what size of upsampling, etc.)\n",
    "- Training : These are the \"outer\" problems, which are not directly related to\n",
    "the architecture of our GAN, but mostly about the training. For example, setting\n",
    "the right learning rate, or choosing the frequency of training of the generator\n",
    "regarding the discriminator, etc.\n",
    "\n",
    "It was hard to understand which architecture was the right one at first. As for\n",
    "many Deep Learning tasks, the questions we ask ourselves are: Which layers?\n",
    "What sizes of layer? Which activation? Which loss? What's the training scheme?\n",
    "Eventually we found lots of information about these questions and understood\n",
    "what could be best for GANs.\n",
    "\n",
    "**Checkerboard artifacts** was the first problem we ran into. It makes grids \n",
    "on the generated images like in the following :\n",
    "\n",
    "![Checkerboard illustration](fig/checkerboard.png)\n",
    "\n",
    "It is a byproduct of the inner workings of transposed convolutions, that make\n",
    "certain pixels values very high or very low, and compounds these effects at\n",
    "each transposed conv layer.\n",
    "\n",
    "To alleviate this effect, we used upsampling + convolutional layers instead of\n",
    "transposed convolutional layers for the generator architecture. This removed\n",
    "immediately all the artifacts.\n",
    "\n",
    "**Scaling up** GANs does not work like we would like it to do. It is not as \n",
    "simple as adding more layers to the network and hoping for the best. We found\n",
    "that we had to change the whole architecture to generate bigger images. New training\n",
    "techniques and architecture have to be used, because many problems arise from\n",
    "the size of the network and of the images we want to generate.\n",
    "\n",
    "The **training losses** are also something we struggled with. In all our \n",
    "experiments, they did not look like what we could see on internet or in the \n",
    "litterature, and we failed to understand why. Especially for the discriminator,\n",
    "our losses are often flat, with peaks when the generator is good at fooling the\n",
    "discriminator. But it seems that the discriminator is always good at \n",
    "recognizing training set pictures.\n",
    "\n",
    "<img src=\"fig/loss.png\" alt=\"Loss and accuracy\" width=\"750\"/>\n",
    "\n",
    "**Mode collapse** is perhaps the most annoying training issue that happened the \n",
    "most in this project. The generator started to generate extremely bad quality\n",
    "images, and always the same type of images. Each entry noise of the network\n",
    "resulted in the same output image. It is a known issue of GAN training, that\n",
    "is hard to avoid.\n",
    "\n",
    "Here is a GIF showing a training that collapsed at first, and then managed to\n",
    "decollapse. We stopped that training because we were using a bilinear mode for\n",
    "the upsampling of the image in our ResBlocks instead of using the nearest\n",
    "neighbors mode, which gives better results. You can see the decollapsing around\n",
    "epoch 20.\n",
    "\n",
    "![Training managing to decollapse](fig/sluggif_collapse.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - What we think could be improved\n",
    "\n",
    "Obviously, we did not achieve what we wanted to achieve in the first place with\n",
    "our GAN : generating real-like landscapes. GANs for medium to big images (128\n",
    "and more) is extremely hard to train, and few architectures succeed in doing so.\n",
    "\n",
    "We did not want to copy an architecture from open-source code, as it would not\n",
    "have been pedagogically interesting. We preferred the process of finding\n",
    "building blocks, tips and ideas from different papers and repositories to get a\n",
    "decent GAN. However, this is a very hard process, because it involves reading\n",
    "papers and understanding state of the art Deep Learning Research from the few\n",
    "past years.\n",
    "\n",
    "We have a few grey areas, questions that we did not find answers online, that remain.\n",
    "- How does the size of the latent space really impact the generation ?\n",
    "- Do we need the generator and discriminator to have the same number of learnable parameters ? Do they need to be strictly \"symmetric\" ?\n",
    "- Why does our GAN rapidly creates images that seem like landscapes from afar, and then adds too much details at the point where it does not seem real at all ?\n",
    "\n",
    "There is a vast litterature about GANs, but we could not find time to read all\n",
    "of them for one project, but some of them could have the answer to why our GAN is not perfect.\n",
    "We could read more about [(Zhang et al)](https://arxiv.org/abs/1805.08318) SAGAN\n",
    "paper, to understand more deeply this layer and what it really does to the training.\n",
    "Our understanding is that is it supposed to give the network more information about\n",
    "global features in order to have more realistic generation of complex objects.\n",
    "It is \"easy\" for a network to learn to generate local features of a dog (an ear,\n",
    "a paw...) but generating a realistic dog with all its parts at the right place\n",
    "is something GANs have struggled to do historically.\n",
    "\n",
    "This self-attention mechanism is also integrable via non-local blocks\n",
    "[(Wang et al)](https://arxiv.org/abs/1711.07971) instead of self-attention\n",
    "layers, and it seems to work in a similar way according to the litterature.\n",
    "\n",
    "\n",
    "The integration of different loss like in Geometric GAN\n",
    "[(Lim and Ye)](https://arxiv.org/abs/1705.02894v2), that have theoretical\n",
    "arguments as for the convergence to Nash equilibrium for a GAN training. Or even\n",
    "using the Wassertein distance as the loss [(Arjovsky et al)](https://arxiv.org/abs/1701.07875),\n",
    "which has proven to be effective for stabilizing training.\n",
    "\n",
    "Moreover, what seems to work best is using many of these state of the art\n",
    "discovery, as is done on the BigGAN paper\n",
    "[(Brock et al)](https://arxiv.org/abs/1809.11096). They use Spectral\n",
    "Normalization to enforce Lipshitz continuity on the discriminator, class\n",
    "information from conditional GANs, self-attention, orthonormal initialization,\n",
    "big batch sizes, truncation trick... It works very well, because they achieved\n",
    "better results than state of the art by orders of magnitude, when it came out in\n",
    "2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataload import *\n",
    "from gan_architecture import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Set a small batch size for the demonstration\n",
    "batch_size = 4\n",
    "\n",
    "#Latent space is a vector of size z_dim\n",
    "z_dim = 128\n",
    "\n",
    "#Model for the notebook\n",
    "file = \"final-gan\"\n",
    "\n",
    "#We only load the generator to generate pictures here, no need for the discriminator\n",
    "model = Generator(z_dim).to(device)\n",
    "model.load_state_dict(torch.load(f\"saved_models/{file}_generator.sav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : DISPLAY THE 4 IMAGES OF BATCH SIZE ON PLT SUBPLOTS\n",
    "\n",
    "fig,axes = plt.subplots(1,2);\n",
    "img1 = (minmax_scale(x, 0, 1))[0].numpy().transpose((1, 2, 0))\n",
    "img2 = minmax_scale(x_test, 0, 1)[0].detach().numpy().transpose((1, 2, 0))\n",
    "axes[0].imshow(img1); # .transpose((1, 2, 0))\n",
    "axes[1].imshow(img2);\n",
    "\n",
    "# x = torch.randn(batch_size, z_dim)\n",
    "# i = np.random.randint(15)\n",
    "# x = x.to(device)\n",
    "# generated = model(x)\n",
    "# print(f\"Shape : {generated.shape}\")\n",
    "# #axes[0].imshow(generated[i].cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "# for i in range(batch_size):\n",
    "#     fig,axes = plt.subplots(1,1, figsize=(8,8))\n",
    "#     axes.imshow(generated[i].cpu().detach().numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on different datasets\n",
    "\n",
    "This is the bonus section, here are a few GIFs showing different evolutions of\n",
    "trainings.\n",
    "\n",
    "We also tried to train our GAN on a dataset of berries, because we thought that\n",
    "it might be simpler to learn for the model (it wasn't). \n",
    "\n",
    "<img src=\"fig/berries.png\" alt=\"Berries dataset\" width=\"550\"/>\n",
    "\n",
    "On the left you can see a training from the beginning using a **bilinear** mode\n",
    "for the upsampling. On the right, we used the **nearest neighbors** mode :\n",
    "\n",
    "![Bilinear berries](fig/sluggif_bilinear_berries.gif) ![Nearest berries](fig/sluggif_nearest_berries.gif)\n",
    "\n",
    "Finally, the next GIF is our last model's training evolution. We can clearly see\n",
    "that, even if the training on the berry dataset did not converge well, the colors\n",
    "are not the same at all than for this last model, which we trained on the landscapes dataset.\n",
    "\n",
    "![Final model](fig/sluggif_final_train.gif)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aff6a0c97bf647d57f829f973db0d341d149cf8adebb692db2ac85668315af91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
